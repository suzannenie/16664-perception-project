{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQTRFG28_DT",
        "outputId": "7ff2de83-8f30-49dc-f3c8-5683c0567469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/16664-my-perception\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/16664-my-perception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moq6mP519Gg6"
      },
      "outputs": [],
      "source": [
        "#! /usr/bin/python3\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import csv\n",
        "\n",
        "images = glob('{}/*/*_image.jpg'.format('16664_perception_final_proj/trainval'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUb8y02Mjrpg",
        "outputId": "e5848674-f8e6-4182-8383-22057bfbcd0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['16664_perception_final_proj',\n",
              " 'trainval',\n",
              " '1431a177-2248-47e2-8709-2628206eeac4',\n",
              " '0033_image.jpg']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'16664_perception_final_proj/trainval/1431a177-2248-47e2-8709-2628206eeac4/0033_image.jpg'.split('/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY50XeWReWKZ"
      },
      "outputs": [],
      "source": [
        "def help(s):\n",
        "  return s[-51:]\n",
        "sorted(images, key=help)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWFJ8ZifeXyJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "labels = pd.read_csv('trainval_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0fGQbXUkuQ2"
      },
      "outputs": [],
      "source": [
        "labels.set_index('guid/image',inplace=True)\n",
        "D = labels.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8IosFWZfYpj"
      },
      "outputs": [],
      "source": [
        "from numpy.core.numeric import identity\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "\n",
        "class ImageDataloader(Dataset):\n",
        "    \"\"\"\n",
        "    Dataloader for Inference.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths, label_dict, target_size=256):\n",
        "\n",
        "        self.img_paths = image_paths\n",
        "        self.label_dict = label_dict\n",
        "        self.target_size = target_size\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.ToPILImage(),transforms.Resize((target_size,target_size)),transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        __getitem__ for inference\n",
        "        :param idx: Index of the image\n",
        "        :return: img_np is a numpy RGB-image of shape H x W x C with pixel values in range 0-255.\n",
        "        And img_tor is a torch tensor, RGB, C x H x W in shape and normalized.\n",
        "        \"\"\"\n",
        "        img = cv2.imread(self.img_paths[idx])\n",
        "        name = self.img_paths[idx]\n",
        "\n",
        "        L = name.split('/')\n",
        "        guid = L[2]\n",
        "        id = L[3][:4]\n",
        "\n",
        "        label = self.label_dict[f'{guid}/{id}']\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Pad images to target size\n",
        "        img_tor = self.transform(img)\n",
        "        # img_tor = img_np.astype(np.float32)\n",
        "        img_tor = img_tor / 255.0\n",
        "        # img_tor = self.normalize(img_tor)\n",
        "\n",
        "        return img_tor, label\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaciISCXlelJ",
        "outputId": "5e00c2e8-a1d7-43d6-a04f-2e40c42f8764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Train Data : 6573\n",
            "Length of Validation Data : 1000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "batch_size = 128\n",
        "val_size = 1000\n",
        "train_size = len(images) - val_size \n",
        "\n",
        "train_data,val_data = random_split(images,[train_size,val_size])\n",
        "print(f\"Length of Train Data : {len(train_data)}\")\n",
        "print(f\"Length of Validation Data : {len(val_data)}\")\n",
        "\n",
        "train_data = ImageDataloader(train_data, D['label'])\n",
        "val_data = ImageDataloader(val_data, D['label'])\n",
        "\n",
        "#load the train and validation into batches.\n",
        "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
        "val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCmwJmhXmC9A"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "  \n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "  \n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
        "    \n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(),lr)\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    \n",
        "    return history\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvAU47LbnMqN"
      },
      "outputs": [],
      "source": [
        "class NaturalSceneClassification(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        \n",
        "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            \n",
        "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            \n",
        "            nn.Flatten(),\n",
        "            nn.Linear(82944,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,6)\n",
        "        )\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "model = NaturalSceneClassification()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ7Nnd4xm8Vf",
        "outputId": "67482d58-d573-443b-efe2-26d4a0ac9070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "num_epochs = 30\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001\n",
        "#fitting the model on training data and record the result after each epoch\n",
        "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ANP0DKEadj6"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4AQlTpmn2I5"
      },
      "outputs": [],
      "source": [
        "#! /usr/bin/python3\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import csv\n",
        "\n",
        "test_images = glob('{}/*/*_image.jpg'.format('16664_perception_final_proj/test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dli3iTHDag1T"
      },
      "outputs": [],
      "source": [
        "names = []\n",
        "for name in test_images:\n",
        "  L = name.split('/')\n",
        "  guid = L[2]\n",
        "  id = L[3][:4]\n",
        "  names.append(f'{guid}/{id}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ssR5zBavNO",
        "outputId": "ae21dd43-0a82-4cc6-faf9-e412ebf94abb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2631"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofqJPqAYa6lQ"
      },
      "outputs": [],
      "source": [
        "with open('test_results1.csv', 'w') as f:\n",
        "  for name in names:\n",
        "    f.write(f'{name},1\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdgGZ9iAbY_j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
